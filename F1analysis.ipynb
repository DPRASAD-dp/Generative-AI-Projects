{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":82253,"databundleVersionId":8965849,"sourceType":"competition"},{"sourceId":8810254,"sourceType":"datasetVersion","datasetId":5299242},{"sourceId":8815510,"sourceType":"datasetVersion","datasetId":5302905},{"sourceId":8815888,"sourceType":"datasetVersion","datasetId":5303163},{"sourceId":186019740,"sourceType":"kernelVersion"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"****EDA Using ydata_profiling***","metadata":{}},{"cell_type":"code","source":"# Make the necessary imports\nimport pandas as pd\nfrom ydata_profiling import ProfileReport\n\n# Load the data\ndf = pd.read_csv(\"/kaggle/input/train-dataset/train.csv\",na_values = \"\\\\N\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-29T04:52:02.080703Z","iopub.execute_input":"2024-06-29T04:52:02.081143Z","iopub.status.idle":"2024-06-29T04:52:46.143521Z","shell.execute_reply.started":"2024-06-29T04:52:02.081106Z","shell.execute_reply":"2024-06-29T04:52:46.142308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate the report\nprofile = ProfileReport(df,title=\"F1 Racer Profile\")\n\n# Save the report to .html\nprofile.to_file(\"f1racer_report.html\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the analysis and the visualisaation is in the f1racer_report.html file","metadata":{}},{"cell_type":"markdown","source":"Handling missing values","metadata":{}},{"cell_type":"code","source":"df.head(15)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T03:17:41.748307Z","iopub.execute_input":"2024-06-29T03:17:41.748751Z","iopub.status.idle":"2024-06-29T03:17:41.785987Z","shell.execute_reply.started":"2024-06-29T03:17:41.748716Z","shell.execute_reply":"2024-06-29T03:17:41.784671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2024-06-29T03:12:18.590888Z","iopub.execute_input":"2024-06-29T03:12:18.591331Z","iopub.status.idle":"2024-06-29T03:12:24.153586Z","shell.execute_reply.started":"2024-06-29T03:12:18.591297Z","shell.execute_reply":"2024-06-29T03:12:24.152106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T04:54:40.931271Z","iopub.execute_input":"2024-06-29T04:54:40.931729Z","iopub.status.idle":"2024-06-29T04:54:40.954806Z","shell.execute_reply.started":"2024-06-29T04:54:40.931690Z","shell.execute_reply":"2024-06-29T04:54:40.953552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming df is your DataFrame\n# Drop columns with 100% missing values\ncols_to_drop = ['fp1_date', 'fp1_time', 'fp2_date', 'fp2_time', 'fp3_date', 'fp3_time', 'quali_date', 'quali_time', 'sprint_date', 'sprint_time']\ndf.drop(columns=cols_to_drop, inplace=True)\n\n# Mean/Median imputation for numerical columns\ndf['position_x'].fillna(df['position_x'].mean(), inplace=True)\ndf['timetaken_in_millisec'].fillna(df['timetaken_in_millisec'].median(), inplace=True)\ndf['max_speed'].fillna(df['max_speed'].mean(), inplace=True)\n\n# Mode imputation for categorical columns\ndf['driver_code'].fillna(df['driver_code'].mode()[0], inplace=True)\n\n# Forward fill for time series columns\ndf['time_x'].fillna(method='ffill', inplace=True)\ndf['time_y'].fillna(method='ffill', inplace=True)\n\n# Create a flag for missing values in 'rank' and then fill with the median\ndf['rank_missing'] = df['rank'].isnull()\ndf['rank'].fillna(df['rank'].median(), inplace=True)\n\n# Predictive imputation can be done using models (not shown here for simplicity)\n\nprint(df.isnull().sum())  # Check remaining missing values\n","metadata":{"execution":{"iopub.status.busy":"2024-06-29T04:53:00.965925Z","iopub.execute_input":"2024-06-29T04:53:00.966340Z","iopub.status.idle":"2024-06-29T04:53:09.604250Z","shell.execute_reply.started":"2024-06-29T04:53:00.966309Z","shell.execute_reply":"2024-06-29T04:53:09.602891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming df is your DataFrame\n\n# Handle `fastestLap` and `fastestLapTime`\n# Example: Fill with a default value -1\ndf['fastestLap'].fillna(-1, inplace=True)\ndf['fastestLapTime'].fillna('00:00:00', inplace=True)  # Assuming the time format is HH:MM:SS\n\n# Handle `time_y` using forward fill as an example\ndf['time_y'].fillna(method='ffill', inplace=True)\n\n# Handle `driver_num` using mode imputation as an example\ndf['driver_num'].fillna(df['driver_num'].mode()[0], inplace=True)\n\nprint(df.isnull().sum())  # Check remaining missing values\n","metadata":{"execution":{"iopub.status.busy":"2024-06-29T04:53:25.031667Z","iopub.execute_input":"2024-06-29T04:53:25.032194Z","iopub.status.idle":"2024-06-29T04:53:31.959312Z","shell.execute_reply.started":"2024-06-29T04:53:25.032157Z","shell.execute_reply":"2024-06-29T04:53:31.958153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Handle `time_y` using a combination of forward fill and backward fill\ndf['time_y'].fillna(method='ffill', inplace=True)\ndf['time_y'].fillna(method='bfill', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T04:53:37.388291Z","iopub.execute_input":"2024-06-29T04:53:37.388708Z","iopub.status.idle":"2024-06-29T04:53:38.454079Z","shell.execute_reply.started":"2024-06-29T04:53:37.388674Z","shell.execute_reply":"2024-06-29T04:53:38.453071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2024-06-29T04:54:57.447935Z","iopub.execute_input":"2024-06-29T04:54:57.448356Z","iopub.status.idle":"2024-06-29T04:55:03.523751Z","shell.execute_reply.started":"2024-06-29T04:54:57.448321Z","shell.execute_reply":"2024-06-29T04:55:03.522514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['number'].fillna(df['number'].mode()[0], inplace=True)\nprint(df.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2024-06-29T04:55:08.187948Z","iopub.execute_input":"2024-06-29T04:55:08.188787Z","iopub.status.idle":"2024-06-29T04:55:14.313295Z","shell.execute_reply.started":"2024-06-29T04:55:08.188750Z","shell.execute_reply":"2024-06-29T04:55:14.312171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Handling duplicate values","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming df is your DataFrame\n\n# Identify all duplicate rows\nduplicates = df.duplicated()\nprint(duplicates.sum())  # Count of duplicate rows\nprint(df[duplicates])","metadata":{"execution":{"iopub.status.busy":"2024-06-29T03:21:49.163081Z","iopub.execute_input":"2024-06-29T03:21:49.164162Z","iopub.status.idle":"2024-06-29T03:21:57.643966Z","shell.execute_reply.started":"2024-06-29T03:21:49.164123Z","shell.execute_reply":"2024-06-29T03:21:57.642564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No duplicate rows found","metadata":{}},{"cell_type":"markdown","source":"Removing unnecessary columns","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming df is your DataFrame\ncols_to_drop = [\n    'fp1_date', 'fp1_time', 'fp2_date', 'fp2_time', 'fp3_date', 'fp3_time',\n    'quali_date', 'quali_time', 'sprint_date', 'sprint_time',\n    'url_x', 'url_y', 'url', 'time_y', 'driver_num', 'raceId_y', 'positionText_x', 'positionText_y', 'driverRef',\n    'fastestLap', 'fastestLapTime'\n]\n\n# Check which columns actually exist in the DataFrame\nexisting_cols_to_drop = [col for col in cols_to_drop if col in df.columns]\n\n# Drop only the existing columns\ndf.drop(columns=existing_cols_to_drop, inplace=True)\n\n# Print the remaining columns to verify\nprint(df.columns)\n\n# Check for remaining missing values\nprint(df.isnull().sum())\n","metadata":{"execution":{"iopub.status.busy":"2024-06-29T04:55:23.914596Z","iopub.execute_input":"2024-06-29T04:55:23.915010Z","iopub.status.idle":"2024-06-29T04:55:28.364347Z","shell.execute_reply.started":"2024-06-29T04:55:23.914980Z","shell.execute_reply":"2024-06-29T04:55:28.362945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T03:30:38.098160Z","iopub.execute_input":"2024-06-29T03:30:38.098589Z","iopub.status.idle":"2024-06-29T03:30:39.784727Z","shell.execute_reply.started":"2024-06-29T03:30:38.098557Z","shell.execute_reply":"2024-06-29T03:30:39.783583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Handling the outliers using IQR technique","metadata":{}},{"cell_type":"code","source":"# Get list of numerical and categorical columns\nnumerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\ncategorical_columns = df.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n\nprint(\"Numerical columns:\", numerical_columns)\nprint(\"Categorical columns:\", categorical_columns)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T04:55:44.758120Z","iopub.execute_input":"2024-06-29T04:55:44.758503Z","iopub.status.idle":"2024-06-29T04:55:45.222882Z","shell.execute_reply.started":"2024-06-29T04:55:44.758474Z","shell.execute_reply":"2024-06-29T04:55:45.221800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming df is your DataFrame with numerical columns as identified\n\n# List of numerical columns\nnumerical_columns = [\n    'resultId', 'racerId', 'driverId', 'constructorId', 'number', 'grid', 'position_x', 'positionOrder', 'points', 'laps', 'timetaken_in_millisec', 'rank', 'max_speed', 'statusId', 'year', 'round', 'circuitId', 'driverStandingsId', 'points_y', 'position', 'wins', 'result_driver_standing'\n]\n\n# Calculate Q1, Q3, and IQR for each numerical column\nQ1 = df[numerical_columns].quantile(0.25)\nQ3 = df[numerical_columns].quantile(0.75)\nIQR = Q3 - Q1\n\n# Define lower and upper bounds to identify outliers\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n# Handling outliers: Replace or remove\nfor col in numerical_columns:\n    # Replace outliers with the nearest non-outlier value\n    df[col] = df[col].mask(df[col] < lower_bound[col], df[col].quantile(0.05))\n    df[col] = df[col].mask(df[col] > upper_bound[col], df[col].quantile(0.95))\n\n# Print the updated DataFrame to verify changes\nprint(df.describe())\n\n# Optionally, you can save the cleaned DataFrame to a CSV file\ndf.to_csv('cleaned_race_data_with_outliers_handled.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-29T04:56:17.112194Z","iopub.execute_input":"2024-06-29T04:56:17.113093Z","iopub.status.idle":"2024-06-29T04:57:34.867247Z","shell.execute_reply.started":"2024-06-29T04:56:17.113049Z","shell.execute_reply":"2024-06-29T04:57:34.865945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_values = df['time_x'].unique()\n\n# Print the unique values\nprint(unique_values)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-29T05:38:40.954383Z","iopub.execute_input":"2024-06-29T05:38:40.955203Z","iopub.status.idle":"2024-06-29T05:38:40.961965Z","shell.execute_reply.started":"2024-06-29T05:38:40.955163Z","shell.execute_reply":"2024-06-29T05:38:40.960703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = pd.read_csv(\"/kaggle/working/cleaned_race_data_with_outliers_handled.csv\")\ndf1.info()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T06:31:19.976166Z","iopub.execute_input":"2024-06-29T06:31:19.976494Z","iopub.status.idle":"2024-06-29T06:31:20.120943Z","shell.execute_reply.started":"2024-06-29T06:31:19.976469Z","shell.execute_reply":"2024-06-29T06:31:20.119911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n\ntrain_df = pd.read_csv(\"/kaggle/input/myyydata/cleaned_race_data_with_outliers_handled.csv\")\nval_df = pd.read_csv(\"/kaggle/input/mydataset/validation/validation.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/mydataset/test/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-06-29T06:42:42.921592Z","iopub.execute_input":"2024-06-29T06:42:42.921940Z","iopub.status.idle":"2024-06-29T06:43:09.919409Z","shell.execute_reply.started":"2024-06-29T06:42:42.921914Z","shell.execute_reply":"2024-06-29T06:43:09.918574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/myyydata/cleaned_race_data_with_outliers_handled.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-06-29T06:43:44.987054Z","iopub.execute_input":"2024-06-29T06:43:44.987439Z","iopub.status.idle":"2024-06-29T06:43:57.947800Z","shell.execute_reply.started":"2024-06-29T06:43:44.987411Z","shell.execute_reply":"2024-06-29T06:43:57.946750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define feature columns and target variable\nX_train = train_df.drop(columns=['position'])\ny_train = train_df['position']\n\nX_val = val_df.drop(columns=['position'])\ny_val = val_df['position']\n","metadata":{"execution":{"iopub.status.busy":"2024-06-29T06:44:02.874503Z","iopub.execute_input":"2024-06-29T06:44:02.874844Z","iopub.status.idle":"2024-06-29T06:44:03.515607Z","shell.execute_reply.started":"2024-06-29T06:44:02.874818Z","shell.execute_reply":"2024-06-29T06:44:03.514777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define numerical and categorical columns\nnumerical_cols = [\"points\",\"timetaken_in_millisec\",\"max_speed\",\"points_y\",\"wins\"]\ncategorical_cols = X_train.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n\n# Define the preprocessing steps for numerical and categorical data\nnumerical_transformer = StandardScaler()\ncategorical_transformer = OneHotEncoder(handle_unknown='ignore')\n\n# Create a ColumnTransformer to apply the preprocessing\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])","metadata":{"execution":{"iopub.status.busy":"2024-06-29T06:44:06.817378Z","iopub.execute_input":"2024-06-29T06:44:06.817720Z","iopub.status.idle":"2024-06-29T06:44:07.193964Z","shell.execute_reply.started":"2024-06-29T06:44:06.817695Z","shell.execute_reply":"2024-06-29T06:44:07.193185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Assuming `preprocessor`, `X_train`, `y_train`, `X_val`, `y_val` are defined and prepared appropriately\n\n# Define Linear Regression model\nlinear_reg = LinearRegression()\n\n# Dictionary to store the best Linear Regression model and its performance\nbest_linear_model = None\nbest_linear_score = None\n\nprint(\"Training Linear Regression...\")\n\n# Create a pipeline that combines the preprocessor with Linear Regression\nlinear_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('regressor', linear_reg)\n])\n\n# Fit the model\nlinear_pipeline.fit(X_train, y_train)\n\n# Evaluate on the validation set\nprint(\"Evaluating Linear Regression on the validation set...\")\ny_val_pred_linear = linear_pipeline.predict(X_val)\nrmse_val_linear = np.sqrt(mean_squared_error(y_val, y_val_pred_linear))\nprint(f\"RMSE on validation set for Linear Regression: {rmse_val_linear}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-29T06:55:14.309808Z","iopub.execute_input":"2024-06-29T06:55:14.310138Z"},"trusted":true},"execution_count":null,"outputs":[]}]}